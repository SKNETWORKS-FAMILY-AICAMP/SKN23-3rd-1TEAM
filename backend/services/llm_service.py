"""
File: llm_service.py
Author: ì–‘ì°½ì¼ (ì´ˆê¸° ë¼ˆëŒ€) / 1íŒ€ (ê³ ë„í™”)
Created: 2026-02-15
Description: LangChain ë° RAG ê¸°ë°˜ AI ë©´ì ‘ê´€ í•µì‹¬ ë‘ë‡Œ ë¡œì§ (ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±)

Modification History:
- 2026-02-15 (ì–‘ì°½ì¼): ì´ˆê¸° ìƒì„± ë° ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± ë¼ˆëŒ€ êµ¬ì¶•
- 2026-02-20 (ê¹€ì§€ìš°): ë¹„ë™ê¸°(Async) ChatOpenAI ì ìš©, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë° RAG(ChromaDB) ì—°ë™ìœ¼ë¡œ ì „ë©´ ê°œí¸
"""

import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from backend.services.rag_service import rag_service  # ğŸ‘ˆ ì‚¬ì„œ(RAG) í˜¸ì¶œ


class LLMService:
    def __init__(self):
        """
        AI ë©´ì ‘ê´€ ì´ˆê¸°í™”: ì•½ê°„ì˜ ì°½ì˜ì„±(temperature)ì„ ì£¼ì–´ ë»”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ì„ ìœ ë„í•©ë‹ˆë‹¤.
        """
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

        # ğŸ”¥ ì••ë°• ë©´ì ‘ê´€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ {job_role} ì§ë¬´ì˜ 10ë…„ ì°¨ ìµœê³ ì°¸ ì‹¤ë¬´ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
            ì§€ì›ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³ , ì•„ë˜ ì œê³µëœ [ì°¸ê³  ë°ì´í„°(ì´ë ¥ì„œ ë° ê³¼ê±° ë‹µë³€)]ë¥¼ ë°”íƒ•ìœ¼ë¡œ 
            ì§€ì›ìì˜ ì§„ì§œ ì‹¤ë ¥ì„ ê²€ì¦í•  ìˆ˜ ìˆëŠ” 'ë‚ ì¹´ë¡œìš´ ê¼¬ë¦¬ ì§ˆë¬¸'ì„ ë”± 1ê°œë§Œ ìƒì„±í•˜ì„¸ìš”.
            
            [ì°¸ê³  ë°ì´í„°]
            {context}
            
            [ë©´ì ‘ê´€ ì§€ì¹¨]
            1. ì§€ì›ìì˜ ë‹µë³€ì´ í›Œë¥­í•˜ë‹¤ë©´, ì‚¬ìš©ëœ ê¸°ìˆ ì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë‚˜ ì‹¬í™” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì„ ë¬¼ì–´ë³´ì„¸ìš”.
            2. ì§€ì›ìì˜ ë‹µë³€ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ëª¨ìˆœì´ ìˆë‹¤ë©´, ê·¸ ë¶€ë¶„ì„ ì˜ˆë¦¬í•˜ê²Œ íŒŒê³ ë“œì„¸ìš”.
            3. ì¸í„°ë„·ì— ë„ë ¤ìˆëŠ” ë»”í•œ ì§ˆë¬¸(ì˜ˆ: ì¥ë‹¨ì ì´ ë­”ê°€ìš”?)ì€ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤.
            4. ëŒ€í™”í•˜ë“¯ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë¡œ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.""",
                ),
                (
                    "human",
                    "ì§€ì›ìì˜ ë°©ê¸ˆ ë‹µë³€: {user_answer}\n\nì´ ë‹µë³€ì— ëŒ€í•œ í‰ê°€ë¥¼ ì†ìœ¼ë¡œ ì§„í–‰í•˜ê³ , ë‹¤ìŒ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ë˜ì ¸ì£¼ì„¸ìš”.",
                ),
            ]
        )

        self.chain = self.prompt | self.llm

    # ==========================================
    # ğŸ¯ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ë©”ì¸ í•¨ìˆ˜ (FastAPI ë¼ìš°í„°ì—ì„œ í˜¸ì¶œ)
    # ==========================================
    async def generate_tail_question(
        self,
        session_id: str,
        job_role: str,
        current_question: str,
        user_answer: str,
        turn: int,
    ):
        """
        ì§€ì›ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ê¼¬ë¦¬ ì§ˆë¬¸ì„ ë¹„ë™ê¸°(Async)ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # 1. ë™ì  RAG: ë°©ê¸ˆ ëŒ€ë‹µì„ DBì— ê¸°ì–µì‹œí‚´
        rag_service.append_interview_log(
            session_id, current_question, user_answer, turn
        )

        # 2. RAG ê²€ìƒ‰: ì´ë ¥ì„œì™€ ê³¼ê±° ëŒ€ë‹µì—ì„œ ë‹¨ì„œ ì°¾ê¸°
        context = rag_service.retrieve_context(session_id, user_answer, k=3)

        # 3. LLM ì¶”ë¡ : í”„ë¡¬í”„íŠ¸ì— ì—®ì–´ì„œ ë°œì‚¬!
        response = await self.chain.ainvoke(
            {"job_role": job_role, "context": context, "user_answer": user_answer}
        )

        return response.content


# FastAPI ë¼ìš°í„°ì—ì„œ ì‰½ê²Œ ê°€ì ¸ë‹¤ ì“¸ ìˆ˜ ìˆë„ë¡ ê°ì²´ ìƒì„±
llm_service = LLMService()


def generate_text(prompt: str) -> str:
    """
    Legacy sync wrapper used by infer router.
    """
    user_answer = (prompt or "").strip()
    if not user_answer:
        return ""

    response = llm_service.chain.invoke(
        {
            "job_role": "Python ë°±ì—”ë“œ ê°œë°œì",
            "context": "",
            "user_answer": user_answer,
        }
    )
    return getattr(response, "content", str(response))
