"""
File: rag_service.py
Author: ê¹€ì§€ìš°
Created: 2026-02-21
Description: ì´ë ¥ì„œ(PDF)ë¥¼ í…ìŠ¤íŠ¸ë¡œ ìª¼ê°œì„œ ChromaDBì— ë„£ê³ , 
             ë©´ì ‘ ì¤‘ ì‹¤ì‹œê°„ ëŒ€í™”ë¥¼ DBì— ì¶”ê°€(Append)í•˜ë©°, 
             í•„ìš”í•  ë•Œ ê´€ë ¨ëœ ë‚´ìš©ì„ ê²€ìƒ‰í•´ì„œ êº¼ë‚´ì£¼ëŠ” ë°ì´í„° ê´€ë¦¬ ì „ë‹´ íŒŒì¼

Modification History:
- 2026-02-21: ì´ˆê¸° ìƒì„±
- 2026-02-22: Vector DB(Chroma) ê´€ë¦¬ + ì‹¤ì‹œê°„ ëŒ€í™” ê¸°ë¡(Append) + GPT-4.1-mini ë©´ì ‘ ë¡œì§ í†µí•©
"""
import os
from openai import OpenAI
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from backend.db import base # ì§ˆë¬¸ DB ì°¸ì¡°ë¥¼ ìœ„í•´ ì¶”ê°€

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class InterviewAIService:
    def __init__(self, persist_dir="./chroma_db"):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.persist_dir = persist_dir
        # ìš”ì²­í•˜ì‹  ëª¨ë¸ ëª…ì¹­ ìœ ì§€
        self.llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.7)
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        self.vector_db = Chroma(persist_directory=self.persist_dir, embedding_function=self.embeddings)

    def ingest_resume(self, file_path: str, session_id: str):
        loader = PyPDFLoader(file_path)
        docs = loader.load_and_split(self.text_splitter)
        for doc in docs:
            doc.metadata = {"source": "resume", "session_id": session_id}
        self.vector_db.add_documents(docs)
        print(f"âœ… [{session_id}] ì´ë ¥ì„œ í•™ìŠµ ì™„ë£Œ")

    # ==========================================
    # ğŸ” ëˆ„ë½ë˜ì—ˆë˜ ê²€ìƒ‰ í•¨ìˆ˜ ì¶”ê°€
    # ==========================================
    def retrieve_context(self, session_id: str, query: str, k: int = 3):
        """ì„¸ì…˜ë³„ ì´ë ¥ì„œ ë° ëŒ€í™” ê¸°ë¡ ê²€ìƒ‰"""
        results = self.vector_db.similarity_search(
            query, k=k, filter={"session_id": session_id}
        )
        return "\n\n".join([f"({res.metadata.get('source')}) {res.page_content}" for res in results])

    # ==========================================
    # ğŸ¯ 500ê°œ ì§ˆë¬¸ DBì™€ ì´ë ¥ì„œ ë§¤ì¹­ ë¡œì§ ì™„ì„±
    # ==========================================
    def get_relevant_question_from_db(self, session_id, db_session, job_role):
        """ì´ë ¥ì„œ ë‚´ìš©ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ê¸°ìˆ  ì§ˆë¬¸ì„ 500ê°œ í’€ì—ì„œ ì°¾ì•„ ë°˜í™˜"""
        # 1. ì´ë ¥ì„œì—ì„œ ê¸°ìˆ  ìŠ¤íƒ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        resume_context = self.retrieve_context(session_id, query="ê¸°ìˆ  ìŠ¤íƒ ë° í”„ë¡œì íŠ¸ ê²½í—˜", k=2)
        
        # 2. DBì—ì„œ í•´ë‹¹ ì§ë¬´ì˜ ì§ˆë¬¸ë“¤ì„ ê°€ì ¸ì˜´
        all_questions = db_session.query(base.QuestionPool).join(base.JobCategory).filter(
            base.JobCategory.target_role == job_role
        ).all()

        if not all_questions:
            return "ê´€ë ¨ ì§ë¬´ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 3. GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë ¥ì„œì™€ ê°€ì¥ ë§¤ì¹­ë˜ëŠ” ì§ˆë¬¸ 1ê°œ ì„ ì •
        q_list = "\n".join([f"- {q.content}" for q in all_questions[:20]]) # ì„±ëŠ¥ìƒ 20ê°œë¡œ ì œí•œ
        prompt = f"ì´ë ¥ì„œ ë‚´ìš©: {resume_context}\n\nì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸:\n{q_list}\n\nìœ„ ì§ˆë¬¸ ì¤‘ ì´ë ¥ì„œì™€ ê°€ì¥ ê´€ë ¨ ê¹Šì€ ì§ˆë¬¸ ë”± í•˜ë‚˜ë§Œ ê³¨ë¼ì„œ ì§ˆë¬¸ ë‚´ìš©ë§Œ ì¶œë ¥í•´."
        
        selected_q = self.llm.invoke(prompt)
        return selected_q.content

    def append_interview_log(self, session_id: str, question: str, user_answer: str):
        log_text = f"[ë©´ì ‘ê´€]: {question}\n[ì§€ì›ì]: {user_answer}"
        metadata = {"source": "live_interview", "session_id": session_id}
        self.vector_db.add_texts(texts=[log_text], metadatas=[metadata])

    def generate_interview_response(self, session_id, user_answer, settings):
        context = self.retrieve_context(session_id, user_answer, k=3)
        prompt = f"""
        ë‹¹ì‹ ì€ {settings['job_role']} ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. (ë‚œì´ë„: {settings['difficulty']})
        ì§€ì›ìì˜ ë‹µë³€: "{user_answer}"
        ì°¸ê³  ì»¨í…ìŠ¤íŠ¸: {context}
        
        ì§€ì‹œì‚¬í•­:
        1. ë‹µë³€ì´ ì´ë ¥ì„œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ë‚ ì¹´ë¡œìš´ ê¸°ìˆ  ê²€ì¦ ì§ˆë¬¸ì„ í•˜ì„¸ìš”.
        2. í˜•ì‹: [ì ìˆ˜] | [ìì‹ ê°] | [í”¼ë“œë°±] | [ë‹¤ìŒì§ˆë¬¸]
        """
        response = self.llm.invoke(prompt)
        return response.content

    def stt_whisper(self, audio_file):
        transcript = client.audio.transcriptions.create(model="whisper-1", file=audio_file)
        return transcript.text

    def tts_voice(self, text: str):
        """í…ìŠ¤íŠ¸ë¥¼ OpenAI TTSë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±(ë°”ì´ë„ˆë¦¬)ìœ¼ë¡œ ë³€í™˜"""
        response = client.audio.speech.create(
            model="tts-1",
            voice="alloy", # ê¸°ë³¸ ë‚¨ì„±/ì—¬ì„± ì¤‘ë¦½ì  ëª©ì†Œë¦¬
            input=text
        )
        return response.content

_instance = None
def get_ai_service():
    global _instance
    if _instance is None:
        _instance = InterviewAIService()
    return _instance