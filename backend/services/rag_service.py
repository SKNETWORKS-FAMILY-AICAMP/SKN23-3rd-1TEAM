"""
File: rag_service.py
Author: ê¹€ì§€ìš°
Created: 2026-02-21
Description: ì´ë ¥ì„œ(PDF)ë¥¼ í…ìŠ¤íŠ¸ë¡œ ìª¼ê°œì„œ ChromaDBì— ë„£ê³ , 
             ë©´ì ‘ ì¤‘ ì‹¤ì‹œê°„ ëŒ€í™”ë¥¼ DBì— ì¶”ê°€(Append)í•˜ë©°, 
             í•„ìš”í•  ë•Œ ê´€ë ¨ëœ ë‚´ìš©ì„ ê²€ìƒ‰í•´ì„œ êº¼ë‚´ì£¼ëŠ” ë°ì´í„° ê´€ë¦¬ ì „ë‹´ íŒŒì¼

Modification History:
- 2026-02-21: ì´ˆê¸° ìƒì„±
"""
# í„°ë¯¸ë„ ì„¤ì¹˜ í•„ìˆ˜: pip install langchain langchain-community langchain-openai chromadb pdfplumber
import os
import pdfplumber
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

class RAGService:
    def __init__(self, persist_dir="./chroma_db"):
        """
        RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: ì„ë² ë”© ëª¨ë¸ê³¼ ë²¡í„° DB(Chroma) ì„¸íŒ…
        """
        # OpenAI API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜(.env)ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.persist_dir = persist_dir
        
        # ChromaDB ì—°ê²°
        self.vector_db = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embeddings
        )
        
        # í…ìŠ¤íŠ¸ ì²­í‚¹(ìª¼ê°œê¸°) ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=50
        )

    # ==========================================
    # ğŸ“„ 1. ì´ë ¥ì„œ PDF ì „ì²˜ë¦¬ ë° DB ì£¼ì… (Init)
    # ==========================================
    def process_resume(self, pdf_file_path: str, session_id: str):
        """
        ì§€ì›ìì˜ ì´ë ¥ì„œë¥¼ íŒŒì‹±í•˜ì—¬ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.
        session_idë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë‹¬ì•„ ë‹¤ë¥¸ ì§€ì›ì ë°ì´í„°ì™€ ì„ì´ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤.
        """
        raw_text = ""
        try:
            with pdfplumber.open(pdf_file_path) as pdf:
                for page in pdf.pages:
                    extracted = page.extract_text()
                    if extracted:
                        raw_text += extracted + "\n"
        except Exception as e:
            print(f"âŒ PDF íŒŒì‹± ì—ëŸ¬: {e}")
            return False

        if not raw_text.strip():
            return False

        chunks = self.text_splitter.split_text(raw_text)
        
        # ë©”íƒ€ë°ì´í„°ì— session_idë¥¼ ë„£ì–´ì„œ 'ëˆ„êµ¬ì˜ ì´ë ¥ì„œì¸ì§€' ì‹ë³„
        metadatas = [{"source": "resume", "session_id": session_id} for _ in chunks]
        
        self.vector_db.add_texts(texts=chunks, metadatas=metadatas)
        print(f"âœ… [{session_id}] ì´ë ¥ì„œ ë°ì´í„° {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ!")
        return True

    # ==========================================
    # ğŸ”„ 2. ë™ì  RAG: ì‹¤ì‹œê°„ ë©´ì ‘ ë‹µë³€ DB ì¶”ê°€
    # ==========================================
    def append_interview_log(self, session_id: str, question: str, user_answer: str, turn: int):
        """
        ë©´ì ‘ ì¤‘ì— ì§€ì›ìê°€ í•œ ëŒ€ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ DBì— ê½‚ì•„ ë„£ìŠµë‹ˆë‹¤. (100ì ì§œë¦¬ ì¹˜íŠ¸í‚¤ ğŸ”¥)
        """
        log_text = f"[ë©´ì ‘ê´€]: {question}\n[ì§€ì›ì]: {user_answer}"
        metadata = {
            "source": "live_interview", 
            "session_id": session_id,
            "turn": turn
        }
        
        self.vector_db.add_texts(texts=[log_text], metadatas=[metadata])
        print(f"ğŸ”„ [{session_id}] {turn}í„´ ì§¸ ëŒ€í™” ê¸°ë¡ ì‹¤ì‹œê°„ ì„ë² ë”© ì™„ë£Œ!")

    # ==========================================
    # ğŸ” 3. RAG ê²€ìƒ‰ (LLMì´ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ë§Œë“¤ê¸° ì „ í˜¸ì¶œ)
    # ==========================================
    def retrieve_context(self, session_id: str, query: str, k: int = 3):
        """
        í˜„ì¬ ì„¸ì…˜(ì§€ì›ì)ì˜ ì´ë ¥ì„œì™€ ê³¼ê±° ë©´ì ‘ ë‹µë³€ ì¤‘, í˜„ì¬ ì§ˆë¬¸(query)ê³¼ ê°€ì¥ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        """
        # DBê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ ë°©ì§€
        if self.vector_db._collection.count() == 0:
            return ""

        # í•´ë‹¹ session_idë¥¼ ê°€ì§„ ë°ì´í„°ë§Œ ê²€ìƒ‰ (ë‹¤ë¥¸ ì‚¬ëŒ ì´ë ¥ì„œ ì„ì„ ë°©ì§€)
        results = self.vector_db.similarity_search(
            query, 
            k=k,
            filter={"session_id": session_id} # ğŸ”¥ ë³´ì•ˆì˜ í•µì‹¬: ë‚´ ë°ì´í„°ë§Œ ê²€ìƒ‰!
        )
        
        # ì°¾ì€ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—®ì–´ì„œ ë°˜í™˜
        context_str = "\n\n".join([f"({res.metadata.get('source')} ì°¸ê³ ) {res.page_content}" for res in results])
        return context_str

# FastAPIì—ì„œ ì–¸ì œë“  ë¶ˆëŸ¬ë‹¤ ì“¸ ìˆ˜ ìˆë„ë¡ ì‹±ê¸€í†¤(Singleton) ê°ì²´ë¡œ í•˜ë‚˜ ë§Œë“¤ì–´ ë‘¡ë‹ˆë‹¤.
rag_service = RAGService()